{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "source:: https://github.com/Jackson-Kang/Pytorch-VAE-tutorial/blob/master/02_Vector_Quantized_Variational_AutoEncoder.ipynb"
      ],
      "metadata": {
        "id": "XeSBYvUE8emU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "96D-3hPiMI06"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torchvision.utils import save_image, make_grid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset_path = '~/datasets'\n",
        "\n",
        "cuda = True\n",
        "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "\n",
        "batch_size = 128\n",
        "img_size = (32, 32) # (width, height)\n",
        "\n",
        "input_dim = 3\n",
        "hidden_dim = 512\n",
        "latent_dim = 16\n",
        "n_embeddings= 512\n",
        "output_dim = 3\n",
        "commitment_beta = 0.25\n",
        "\n",
        "lr = 2e-4\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "print_step = 50"
      ],
      "metadata": {
        "id": "08mul5Z88UQ-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import CIFAR10\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "mnist_transform=transforms.Compose([ # 여러 변환 순차적 적용\n",
        "    transforms.ToTensor()\n",
        "    ])\n",
        "kwargs={'num_workers': 1, 'pin_memory': True}\n",
        "train_dataset=CIFAR10(dataset_path, transform=mnist_transform, train=True, download=True) # train=True → 학습용 데이터셋 (training set) 을 불러오라는 의미\n",
        "test_dataset  = CIFAR10(dataset_path, transform=mnist_transform, train=False, download=True)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, **kwargs) # 키워드 인자들을 딕셔너리처럼 받아옴\n",
        "test_loader  = DataLoader(dataset=test_dataset,  batch_size=batch_size, shuffle=False,  **kwargs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQUTRIVb8ike",
        "outputId": "43189b8e-d0a1-4fbb-9542-ef514027775e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 39.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input, hidden, out, kernel_size=(4,4,3,1), stride=2):\n",
        "    super().__init__()\n",
        "\n",
        "    kernel1, kernel2, kernel3, kernel4= kernel_size\n",
        "    self.conv1=nn.Conv2d(input, hidden, kernel1, stride, padding=1)\n",
        "    # nn.Conv2d(input_dim, hidden_dim는 채널 수, out 채널 수만큼 독립적인 hidden개의 커널이 생성됨 (사이즈는 kernel1)\n",
        "    self.conv2=nn.Conv2d(hidden, hidden, kernel2, stride, padding=1)\n",
        "\n",
        "    self.residualconv1=nn.Conv2d(hidden, hidden, kernel3, padding=1)\n",
        "    self.residualconv2=nn.Conv2d(hidden, hidden, kernel4, padding=0)\n",
        "\n",
        "    self.proj=nn.Conv2d(hidden, out, kernel_size=1) # 해상도는 유지하며 채널을 out개로 만듬\n",
        "\n",
        "  def forward(self, x):\n",
        "    x=self.conv1(x)\n",
        "    x=self.conv2(x)\n",
        "    x=F.relu(x)\n",
        "\n",
        "    y=self.residualconv1(x)\n",
        "    y=y+x\n",
        "\n",
        "    x=F.relu(y)\n",
        "    y=self.residualconv2(x)\n",
        "    y=y+x\n",
        "\n",
        "    y=self.proj(y)\n",
        "    return y\n"
      ],
      "metadata": {
        "id": "f-jlTbOGAgQB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VQEmbeddingEMA(nn.Module): # 코드북(임베딩 테이블) 학습\n",
        "  def __init__(self, n_embeddings, embedding_dim, commitment_cost=0.25, decay=0.999, epsilon=1e-5):\n",
        "    super().__init__()\n",
        "    self.commitment_cost=commitment_cost\n",
        "    self.decay=decay\n",
        "    self.epsilon=epsilon\n",
        "\n",
        "    init_bound = 1 / n_embeddings\n",
        "    embedding = torch.Tensor(n_embeddings, embedding_dim)\n",
        "    embedding.uniform_(-init_bound, init_bound)    # Uniform distribution에 따라 제자리에서 초기화\n",
        "    '''\n",
        "    코드북을 학습하기 때문에 적절한 값으로 초기화하지 않으면 학습 초기에 코드북 벡터들이 지나치게 크거나 유사해서 학습\n",
        "    이 불안정해짐 - 작은 값의 균등분포로 초기\n",
        "    '''\n",
        "    self.register_buffer(\"embedding\", embedding) # 코드북 임베딩은 일반적인 gradient descent로 학습되지 않음\n",
        "    self.register_buffer(\"ema_count\", torch.zeros(n_embeddings)) # EMA방식은 직접 수식으로 업데이트\n",
        "    self.register_buffer(\"ema_weight\", self.embedding.clone())\n",
        "\n",
        "  def encode(self, x):\n",
        "    '''\n",
        "    x를 입력으로 받아서 코드북 참조해서 가장 가까운 인덱스로 양자화하고\n",
        "    그 벡터(코드북)와 인덱스 반환\n",
        "    '''\n",
        "    M, D=self.embedding.size()\n",
        "    x_flat= x.detach().reshape(-1,D) # x.detach(): 해당 텐서에서 기울기 흐름 끊음\n",
        "\n",
        "    distances=torch.cdist(x_flat, self.embedding, p=2) # .cdist: 각 x_flat[i]와 코드북의 모든 벡터 간 거리 계산\n",
        "\n",
        "    indices=torch.argmin(distances.float(), dim=1) # 가장 가까운 코드북벡터 찾기\n",
        "    quantized = F.embedding(indices, self.embedding) # 인덱스들을 통해 각 위치에 대응하는 코드북 벡터들 뽑아냄\n",
        "    quantized = quantized.view_as(x)                 # 다시 x shape대로 복원\n",
        "    return quantized, indices.view(x.shape[0], x.shape[2], x.shape[3])  # 양자화된 출력 텐서 (B, D, H, W)와 각 위치에 어떤 코드북 인덱스 사용했는지\n",
        "\n",
        "  def retrieve_random_codebook(self, random_indices):\n",
        "    quantized = F.embedding(random_indices, self.embedding)\n",
        "    quantized = quantized.permute(0, 3, 1, 2) # 텐서의 차원 순서를 바꾸는 함수\n",
        "\n",
        "    return quantized\n",
        "\n",
        "  def forward(self, x):\n",
        "    M,D=self.embedding.size() # M개 코드, D 임베딩벡터크기\n",
        "    x_flat=x.detach().reshape(-1,D)\n",
        "\n",
        "    distances=torch.cdist(x_flat, self.embedding,p=2)\n",
        "\n",
        "    indices=torch.argmin(distances.float(), dim=1)\n",
        "    encodings=F.one_hot(indices, M).float() # 양자화된 코드북 인덱스를 원핫벡터로 / 각 코드북 벡터가 얼마나 선택되었는지 카운트\n",
        "\n",
        "    quantized=F.embedding(indices, self.embedding)\n",
        "    quantized=quantized.view_as(x)\n",
        "\n",
        "    if self.training:\n",
        "      self.ema_count = self.decay * self.ema_count + (1 - self.decay) * torch.sum(encodings, dim=0)\n",
        "      n = torch.sum(self.ema_count)\n",
        "      self.ema_count = (self.ema_count + self.epsilon) / (n + M * self.epsilon) * n\n",
        "\n",
        "      dw = torch.matmul(encodings.t(), x_flat)\n",
        "      self.ema_weight = self.decay * self.ema_weight + (1 - self.decay) * dw\n",
        "      self.embedding = self.ema_weight / self.ema_count.unsqueeze(-1)\n",
        "\n",
        "    codebook_loss=F.mse_loss(x.detach(),quantized) # codebook만 학습\n",
        "    e_latent_loss=F.mse_loss(x, quantized.detach()) # encoder만 학습\n",
        "    commitment_loss=self.commitment_cost*e_latent_loss # scale된 encoder 손실\n",
        "\n",
        "    quantized = x + (quantized - x).detach() # gradient override를 위한 테크닉- encoder 쪽에는 gradient를 흘리되, codebook 쪽에는 gradient가 안 흐르도록\n",
        "\n",
        "    avg_probs = torch.mean(encodings, dim=0) # 각 코드북 벡터의 선택 확률(avg_probs)\n",
        "    perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10))) # 코드북 사용의 다양성을 측정\n",
        "\n",
        "    return quantized, commitment_loss, codebook_loss, perplexity\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K2CZfb5aPTG5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, kernel_sizes=(1, 3, 2, 2), stride=2):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        kernel_1, kernel_2, kernel_3, kernel_4 = kernel_sizes\n",
        "\n",
        "        self.in_proj = nn.Conv2d(input_dim, hidden_dim, kernel_size=1)\n",
        "\n",
        "        self.residual_conv_1 = nn.Conv2d(hidden_dim, hidden_dim, kernel_1, padding=0)\n",
        "        self.residual_conv_2 = nn.Conv2d(hidden_dim, hidden_dim, kernel_2, padding=1)\n",
        "\n",
        "        self.strided_t_conv_1 = nn.ConvTranspose2d(hidden_dim, hidden_dim, kernel_3, stride, padding=0)\n",
        "        self.strided_t_conv_2 = nn.ConvTranspose2d(hidden_dim, output_dim, kernel_4, stride, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.in_proj(x)\n",
        "\n",
        "        y = self.residual_conv_1(x)\n",
        "        y = y+x\n",
        "        x = F.relu(y)\n",
        "\n",
        "        y = self.residual_conv_2(x)\n",
        "        y = y+x\n",
        "        y = F.relu(y)\n",
        "\n",
        "        y = self.strided_t_conv_1(y)\n",
        "        y = self.strided_t_conv_2(y)\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "WTmvDIjWgrCs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, Encoder, Codebook, Decoder):\n",
        "        super(Model, self).__init__()\n",
        "        self.encoder = Encoder\n",
        "        self.codebook = Codebook\n",
        "        self.decoder = Decoder\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        z_quantized, commitment_loss, codebook_loss, perplexity = self.codebook(z)\n",
        "        x_hat = self.decoder(z_quantized)\n",
        "\n",
        "        return x_hat, commitment_loss, codebook_loss, perplexity"
      ],
      "metadata": {
        "id": "IfEpxNrihrZ9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(input=input_dim, hidden=hidden_dim, out=latent_dim)\n",
        "codebook = VQEmbeddingEMA(n_embeddings=n_embeddings, embedding_dim=latent_dim)\n",
        "decoder = Decoder(input_dim=latent_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
        "\n",
        "model = Model(Encoder=encoder, Codebook=codebook, Decoder=decoder).to(DEVICE)"
      ],
      "metadata": {
        "id": "ksRcvdIUhsuU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "mse_loss = nn.MSELoss()\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "Hj6tW9E6hvsY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    overall_loss = 0\n",
        "    for batch_idx, (x, _) in enumerate(train_loader):\n",
        "        x = x.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x_hat, commitment_loss, codebook_loss, perplexity = model(x)\n",
        "        recon_loss = mse_loss(x_hat, x)\n",
        "\n",
        "        loss =  recon_loss + commitment_loss * commitment_beta + codebook_loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % print_step ==0:\n",
        "            print(\"epoch:\", epoch + 1, \"(\", batch_idx + 1, \") recon_loss:\", recon_loss.item(), \" perplexity: \", perplexity.item(),\n",
        "              \" commit_loss: \", commitment_loss.item(), \"\\n\\t codebook loss: \", codebook_loss.item(), \" total_loss: \", loss.item(), \"\\n\")\n",
        "\n",
        "print(\"Finish!!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "UUsqQ3W6hx9m",
        "outputId": "64b1e905-9e37-4048-cccf-edc061b2141a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 ( 1 ) recon_loss: 0.5390774011611938  perplexity:  14.41859245300293  commit_loss:  0.0031496435403823853 \n",
            "\t codebook loss:  0.012598574161529541  total_loss:  0.5524634122848511 \n",
            "\n",
            "epoch: 1 ( 51 ) recon_loss: 0.03720606490969658  perplexity:  25.929798126220703  commit_loss:  0.04992463439702988 \n",
            "\t codebook loss:  0.1996985375881195  total_loss:  0.2493857592344284 \n",
            "\n",
            "epoch: 1 ( 101 ) recon_loss: 0.023040538653731346  perplexity:  93.21887969970703  commit_loss:  0.03653959929943085 \n",
            "\t codebook loss:  0.1461583971977234  total_loss:  0.1783338338136673 \n",
            "\n",
            "epoch: 1 ( 151 ) recon_loss: 0.017749948427081108  perplexity:  162.18373107910156  commit_loss:  0.031406134366989136 \n",
            "\t codebook loss:  0.12562453746795654  total_loss:  0.1512260138988495 \n",
            "\n",
            "epoch: 1 ( 201 ) recon_loss: 0.01515465509146452  perplexity:  203.0373077392578  commit_loss:  0.028093479573726654 \n",
            "\t codebook loss:  0.11237391829490662  total_loss:  0.13455194234848022 \n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-fd0abfec5e8f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mrecon_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcommitment_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcommitment_beta\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcodebook_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}